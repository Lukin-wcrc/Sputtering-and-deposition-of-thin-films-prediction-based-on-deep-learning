{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433b2188-39c2-485e-a785-00d069e3fa92",
   "metadata": {},
   "source": [
    "# library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64650b18-3e4b-4357-9a8f-6e057434db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "\n",
    "# 스케일 처리\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import  MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, Flatten, Dense, MaxPooling1D, Reshape\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea11f80-5ae6-4d54-8349-db3b51a86872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_2372\\3148714721.py:6: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import IPython\n",
    "\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40635087-254e-4332-bd11-566175d3637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    seed = 42\n",
    "    device = \"cuda:0\"    \n",
    "        \n",
    "    lr = 1e-3\n",
    "    epochs = 25\n",
    "    batch_size = 32\n",
    "    num_workers = 4\n",
    "    train_5_folds = True\n",
    "    \n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebeec3b-73b9-42dc-a119-96b22f06a9b7",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bd960bc-0429-4c4e-b730-0cfeb4843921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 308) (40, 176) (40, 132) (36, 308) (36, 176) (36, 132)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "B7_TM_re2 = pd.read_csv('../data/B7/B7_Al_TM_5x8.csv')\n",
    "B7_MG = pd.read_csv('../data/B7/B7_Al_MG.csv')\n",
    "V3_TM_re2 = pd.read_csv('../data/V3/V3_Al_TM_5x8.csv')\n",
    "V3_MG = pd.read_csv('../data/V3/V3_Al_MG.csv')\n",
    "\n",
    "# B7과 V3데이터 합치기\n",
    "TM = pd.concat([B7_TM_re2 ,V3_TM_re2 ], axis=1)\n",
    "MG = pd.concat([B7_MG ,V3_MG ], axis=1)\n",
    "\n",
    "MG[MG == 755.0] = 75.5\n",
    "\n",
    "print(TM.shape, B7_TM_re2.shape, V3_TM_re2.shape, MG.shape, B7_MG.shape, V3_MG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927a91e0-e836-4a61-b422-2b083251a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일 변환\n",
    "def X_scale(X):\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled= scaler.fit_transform(X)\n",
    "    X_scaled = pd.DataFrame(X_scaled)\n",
    "    return X_scaled\n",
    "\n",
    "# 이상치 정리\n",
    "def outlier(X):\n",
    "    q1 = X.quantile(0.25)\n",
    "    q3 = X.quantile(0.75)\n",
    "    IQR = q3-q1\n",
    "    condition = (X > (q1 - 1.5*IQR )) & (X < (q3 + 1.5*IQR ))\n",
    "    X_drop = X[condition]\n",
    "    X_fill = X_drop.fillna(X_drop.interpolate()) # Nan값을 앞뒤의 값의 동일 간격으로 채우기\n",
    "    X_fill = X_fill.fillna(method='ffill')\n",
    "    X_fill = X_fill.fillna(method='bfill')\n",
    "    return X_fill\n",
    "\n",
    "def X_array(X):\n",
    "    N = X.shape[1]\n",
    "    X_re = np.zeros((N, 40, 1))\n",
    "    for k in range(N):\n",
    "        X_re[k, :, 0]  = X.T.iloc[k,:]\n",
    "    return X_re\n",
    "\n",
    "def X_array_2D(X):\n",
    "    N = X.shape[1]\n",
    "    X_re = np.zeros((N, 5, 8))\n",
    "    for k in range(N):\n",
    "        for i in range(5):\n",
    "            X_re[k, i,: ]  = X.T.iloc[k,i*5:i*5+8]\n",
    "    return X_re\n",
    "\n",
    "\n",
    "def Y_array(Y):\n",
    "    N = Y.shape[1]\n",
    "    Y_re = np.zeros((N,36))\n",
    "    for k in range(N):\n",
    "        Y_re[k, :] = Y.T.iloc[k,:]\n",
    "    return Y_re\n",
    "\n",
    "# y 스케일 변환\n",
    "def Y_scale(Y): \n",
    "    scaler = StandardScaler()\n",
    "    Y_scaled = scaler.fit_transform(Y)\n",
    "    # Y_scaled = pd.DataFrame(Y_scaled)\n",
    "    return scaler, Y_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0811091-dc43-4bb6-aa37-aa8a2543877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일 처리 및 이상치 제거\n",
    "X_scaled = X_scale(TM)\n",
    "X_outliner = outlier(X_scaled)\n",
    "X = X_array(X_outliner)\n",
    "X_2D = X_array_2D(X_outliner)\n",
    "\n",
    "Y_array = Y_array(MG)\n",
    "scaler, Y = Y_scale(Y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce8ede8-4e8b-4bed-a066-382512a44b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X의 shape:  (308, 40, 1) , Y의 shape: (308, 36)\n"
     ]
    }
   ],
   "source": [
    "print('X의 shape: ', X.shape ,', Y의 shape:',Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d8a8a2-f07a-489f-9cf3-5a7989087809",
   "metadata": {},
   "source": [
    "# 기타 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d771d8e-11ba-44ec-b234-a1d41b08c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pred_plot(list,y_test, y_pred): \n",
    "\n",
    "    x_ax = range(36)\n",
    "\n",
    "    plt.subplots(constrained_layout=True)\n",
    "    for i,k in zip(range(1,5),list):\n",
    "        plt.subplot(2,2,i)\n",
    "        plt.plot(x_ax, y_test[k], label=\"test\", color='y')\n",
    "        plt.plot(x_ax, y_pred[k], label=\"pred\", color='c')\n",
    "        plt.legend()\n",
    "        plt.title(k)\n",
    "        plt.ylabel('MG Height')\n",
    "        plt.xlabel('location')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e778a4-c064-4656-8fb5-e92978191d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pred_mean_plot(y_test, y_pred): \n",
    "    x_ax = range(36)\n",
    "    plt.plot(x_ax, pd.DataFrame(y_test).mean(), label=\"test_mean\", color='r')\n",
    "    plt.plot(x_ax, pd.DataFrame(y_pred).mean(), label=\"pred_mean\", color='b')\n",
    "    plt.legend()\n",
    "    plt.ylabel('MG Height')\n",
    "    plt.xlabel('location')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4158ed26-d913-4b61-8886-4bfbfc16a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일 역변환\n",
    "def scaler_inv(y_test, y_pred):\n",
    "    y_test_rev = scaler.inverse_transform(y_test)\n",
    "    df_y_test = pd.DataFrame(y_test_rev.T)\n",
    "    y_pred_rev = scaler.inverse_transform(y_pred)\n",
    "    df_y_pred = pd.DataFrame(y_pred_rev.T)\n",
    "    return y_test_rev, y_pred_rev, df_y_test, df_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3bf821-3c09-469e-91d5-f3745492765e",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b923f25f-4b9e-4897-afd8-704e1e3a81c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train의 shape:  (246, 40, 1) , X_test의 shape: (62, 40, 1) , y_train의 shape:  (246, 36) , y_test의 shape: (62, 36)\n",
      "X_train의 shape:  (196, 40, 1) , X_test의 shape: (50, 40, 1) , y_train의 shape:  (196, 36) , y_test의 shape: (50, 36)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print('X_train의 shape: ', X_train.shape ,', X_test의 shape:',X_test.shape,\n",
    "     ', y_train의 shape: ', y_train.shape ,', y_test의 shape:',y_test.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print('X_train의 shape: ', X_train.shape ,', X_test의 shape:',X_val.shape,\n",
    "     ', y_train의 shape: ', y_train.shape ,', y_test의 shape:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c605b7c-557b-49e0-a1e7-cfcafa77e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_MLP(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(40,1)))\n",
    "\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Choose an optimal value between 32-512\n",
    "    hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n",
    "    model.add(keras.layers.Dense(units = hp.Int('units_1', min_value = 32, max_value = 512, step = 32)\n",
    "                                 , activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(units = hp.Int('units_2', min_value = 32, max_value = 512, step = 32)\n",
    "                                 , activation = 'relu'))\n",
    "    model.add(keras.layers.Dense(units = hp.Int('units_3', min_value = 32, max_value = 512, step = 32)\n",
    "                                 , activation = 'relu'))\n",
    "    '''\n",
    "    model.add(keras.layers.Dense(units = hp.Int('units_4', min_value = 32, max_value = 512, step = 32)\n",
    "                                 , activation = 'relu'))\n",
    "    '''\n",
    "    model.add(keras.layers.Dense(36, activation=keras.layers.LeakyReLU(alpha=hp.Float('alpha', min_value=0.00, max_value=0.5, step=0.05))))\n",
    "\n",
    "    # Tune the learning rate for the optimizer \n",
    "    model.summary()\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b8738f-c905-4b93-89f6-b7a06b205c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 40)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                1312      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 36)                1188      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,612\n",
      "Trainable params: 4,612\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_MLP,\n",
    "                     objective = 'val_loss', \n",
    "                     max_epochs = 10,\n",
    "                     overwrite=True,\n",
    "                     factor = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d049819a-22d9-4100-bccf-822ee1eed78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 02s]\n",
      "val_loss: 0.6926265358924866\n",
      "\n",
      "Best val_loss So Far: 0.6926265358924866\n",
      "Total elapsed time: 00h 00m 33s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs = 100, validation_data = (X_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=3)])\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "342c13b9-e26c-4a54-9b46-028918b99457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "units_1 : 416,\n",
      "units_2 : 416,\n",
      "units_3 : 480\n",
      "\n",
      "\n",
      "alpha : 0.30000000000000004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "units_1 : {best_hps.get('units_1')},\n",
    "units_2 : {best_hps.get('units_2')},\n",
    "units_3 : {best_hps.get('units_3')}\n",
    "\n",
    "\n",
    "alpha : {best_hps.get('alpha')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9fecba4-8107-457e-baf8-82cece913b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = tuner.hypermodel.build(best_hps)\\nmodel.summary()\\nhistory = model.fit(X_train, y_train, epochs = 200, validation_data = (X_test, y_test))\\n\\npd.DataFrame(history.history)[['loss','val_loss']].plot()\\nplt.title('Loss')\\nplt.show()\\n\\ny_pred = model.predict(X_test)\\n\\nprint('mae:',mean_absolute_error(y_test, y_pred), 'rmse:',mean_squared_error(y_test, y_pred), 'r2:',r2_score(y_test, y_pred))\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "'''\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, epochs = 200, validation_data = (X_test, y_test))\n",
    "\n",
    "pd.DataFrame(history.history)[['loss','val_loss']].plot()\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print('mae:',mean_absolute_error(y_test, y_pred), 'rmse:',mean_squared_error(y_test, y_pred), 'r2:',r2_score(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446ff66-dadb-492c-91a1-e582d78520ca",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27d4caf6-83c0-44fe-954a-82b621b52d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train의 shape:  (246, 40, 1) , X_test의 shape: (62, 40, 1) , y_train의 shape:  (246, 36) , y_test의 shape: (62, 36)\n",
      "X_train의 shape:  (196, 40, 1) , X_test의 shape: (50, 40, 1) , y_train의 shape:  (196, 36) , y_test의 shape: (50, 36)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print('X_train의 shape: ', X_train.shape ,', X_test의 shape:',X_test.shape,\n",
    "     ', y_train의 shape: ', y_train.shape ,', y_test의 shape:',y_test.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print('X_train의 shape: ', X_train.shape ,', X_test의 shape:',X_val.shape,\n",
    "     ', y_train의 shape: ', y_train.shape ,', y_test의 shape:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c4da726-33e4-4b4f-927b-e6035b2f0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers.experimental import RandomFourierFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02101bbc-02af-495e-bb60-65fa72e9debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_SVM(hp):\n",
    "    n_timesteps = 40\n",
    "    n_features = 1\n",
    "    n_outputs = 36\n",
    "    \n",
    "    # create model object\n",
    "    model = keras.Sequential()\n",
    "    model.add(Reshape((40,), input_shape=(n_timesteps,n_features)))\n",
    "    model.add(keras.Input(shape=(40,)))\n",
    "    model.add(RandomFourierFeatures(output_dim=hp.Int('output_dim', min_value=32, max_value=1280, step=16),\n",
    "                                    scale=hp.Int('scale', min_value=1, max_value=20, step=1),\n",
    "                                    kernel_initializer=hp.Choice('kernal', values = ['Laplacian','Gaussian']))\n",
    "                                    #kernel_initializer=hp.Choice('kernal', values = ['Gaussian']))\n",
    "             )\n",
    "    #model.add(Dense(n_outputs, activation=keras.layers.ELU(alpha=hp.Float('alpha', min_value=0.00, max_value=5, step=0.05)))) # , activation=None\n",
    "    model.add(keras.layers.Dense(36, activation=keras.layers.LeakyReLU(alpha=hp.Float('alpha', min_value=0.00, max_value=0.5, step=0.05))))\n",
    "    #compilation of model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cedd0fab-79ea-4844-8905-2831deb49a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_SVM,\n",
    "                     objective = 'val_loss', \n",
    "                     max_epochs = 10,\n",
    "                     overwrite=True,\n",
    "                     factor = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f684151f-a80d-432e-be90-2ace3fc7d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 02s]\n",
      "val_loss: 1.0578862428665161\n",
      "\n",
      "Best val_loss So Far: 0.8066952228546143\n",
      "Total elapsed time: 00h 00m 29s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs = 100, batch_size=24,  validation_data = (X_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=20)])\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e65f992b-a1e3-4500-a163-24cdf3f3e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output_dim : 944\n",
      "scale : 14\n",
      "kernal : Laplacian\n",
      "\n",
      "alpha : 0.30000000000000004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "output_dim : {best_hps.get('output_dim')}\n",
    "scale : {best_hps.get('scale')}\n",
    "kernal : {best_hps.get('kernal')}\n",
    "\n",
    "alpha : {best_hps.get('alpha')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca80f7-d714-4a64-9fb1-2ecc599478b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e505f2ca-06e5-4d8b-b3a4-b0efd8dc020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train의 shape:  (246, 40, 1) , X_test의 shape: (62, 40, 1) , y_train의 shape:  (246, 36) , y_test의 shape: (62, 36)\n",
      "X_train의 shape:  (196, 40, 1) , X_test의 shape: (50, 40, 1) , y_train의 shape:  (196, 36) , y_test의 shape: (50, 36)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print('X_train의 shape: ', X_train.shape ,', X_test의 shape:',X_test.shape,\n",
    "     ', y_train의 shape: ', y_train.shape ,', y_test의 shape:',y_test.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print('X_train의 shape: ', X_train.shape ,', X_test의 shape:',X_val.shape,\n",
    "     ', y_train의 shape: ', y_train.shape ,', y_test의 shape:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f6d82f2-5e39-474a-9b47-ea89251c44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dropout, Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization, Activation\n",
    "\n",
    "def model_1D_CNN(hp):\n",
    "    # create model object\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    input_shape =(40,1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [2,3,4]),\n",
    "        # activation='relu',\n",
    "        input_shape = input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('conv_2_filter', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_2_kernel', values = [2,3,4]),\n",
    "        # activation='relu',\n",
    "        padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    '''\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Int('conv_3_filter', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_3_kernel', values = [2,3,4]),\n",
    "        # activation='relu',\n",
    "        padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=hp.Int('MaxPooling2D', min_value=1, max_value=5, step=1)\n",
    "                          ))\n",
    "    '''\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_1', min_value=32, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('Dropout', min_value=0.00, max_value=0.5, step=0.05)))    \n",
    "        \n",
    "\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_2', min_value=32, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dense(36,\n",
    "                     activation=keras.layers.LeakyReLU(alpha=hp.Float('alpha', min_value=0.00, max_value=0.5, step=0.05))\n",
    "                    ))\n",
    "    \n",
    "    #compilation of modelv\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "058fc3fc-e398-45ab-a081-1cb2b1870d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_1D_CNN,\n",
    "                     objective = 'val_loss', \n",
    "                     max_epochs = 10,\n",
    "                     overwrite=True,\n",
    "                     factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81e20934-4cb6-4da7-8fdf-ba422ae174fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 Complete [00h 00m 03s]\n",
      "val_loss: 1.3048826456069946\n",
      "\n",
      "Best val_loss So Far: 1.2325024604797363\n",
      "Total elapsed time: 00h 00m 32s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs = 100, batch_size=12,  validation_data = (X_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=20)])\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eba780c4-e059-40e3-9fb0-8257f53c5387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv_1 : 80\n",
      "conv_1_kernel : 2\n",
      "\n",
      "dense_1 : 80\n",
      "Dropout : 0.15000000000000002\n",
      "dense_2 : 64\n",
      "alpha : 0.25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "conv_1 : {best_hps.get('conv_1_filter')}\n",
    "conv_1_kernel : {best_hps.get('conv_1_kernel')}\n",
    "\n",
    "dense_1 : {best_hps.get('dense_1')}\n",
    "Dropout : {best_hps.get('Dropout')}\n",
    "dense_2 : {best_hps.get('dense_2')}\n",
    "alpha : {best_hps.get('alpha')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7169676-4ffa-4695-b187-77b717799cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1D_CNN(hp):\n",
    "    # create model object\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # adding first convolutional layer \n",
    "    model.add(keras.layers.Conv1D(\n",
    "        filters= hp.Int('conv_1', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n",
    "        activation='relu',\n",
    "        input_shape=(40,1)\n",
    "    ))\n",
    "    '''\n",
    "    # adding second convolutional layer \n",
    "    model.add(keras.layers.Conv1D(\n",
    "        filters= hp.Int('conv_2', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "\n",
    "    # adding third convolutional layer \n",
    "    model.add(keras.layers.Conv1D(\n",
    "        filters= hp.Int('conv_3', min_value=32, max_value=64, step=16),\n",
    "        kernel_size=hp.Choice('conv_3_kernel', values = [3,5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "\n",
    "    # adding pooling layer \n",
    "    model.add(MaxPooling1D(pool_size=hp.Int('MaxPooling1D', min_value=1, max_value=5, step=1)\n",
    "                          ))\n",
    "    '''\n",
    "\n",
    "    # adding flatten layer    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_1', min_value=32, max_value=128, step=16),\n",
    "        activation= 'relu'\n",
    "    ))\n",
    "    # adding dropout layer \n",
    "    model.add(Dropout(hp.Float('Dropout', min_value=0.00, max_value=0.5, step=0.05)))\n",
    "    \n",
    "    # adding dense layer    \n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_2', min_value=32, max_value=128, step=16),\n",
    "        activation= 'relu'\n",
    "    ))\n",
    "    # output layer    \n",
    "    model.add(Dense(36, keras.layers.LeakyReLU(alpha=hp.Float('alpha', min_value=0.00, max_value=0.5, step=0.05))))\n",
    "    \n",
    "    #compilation of model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26b7dfef-221f-4992-89df-6abf6bde58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_1D_CNN,\n",
    "                     objective = 'val_loss', \n",
    "                     max_epochs = 10,\n",
    "                     overwrite=True,\n",
    "                     factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdd0e675-0039-4ca3-8493-543eeb191e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 Complete [00h 00m 03s]\n",
      "val_loss: 0.9670314788818359\n",
      "\n",
      "Best val_loss So Far: 0.8154051303863525\n",
      "Total elapsed time: 00h 00m 21s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs = 100, batch_size=12,  validation_data = (X_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=20)])\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7e44f45-a2a7-40f9-8d06-1e671d341d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv_1_filter : 80\n",
      "conv_1_kernel : 5\n",
      "\n",
      "\n",
      "dense_1 : 112\n",
      "Dropout : 0.25\n",
      "\n",
      "dense_2 : 96\n",
      "alpha : 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 배치 정규화 버전\n",
    "print(f\"\"\"\n",
    "conv_1_filter : {best_hps.get('conv_1')}\n",
    "conv_1_kernel : {best_hps.get('conv_1_kernel')}\n",
    "\n",
    "\n",
    "dense_1 : {best_hps.get('dense_1')}\n",
    "Dropout : {best_hps.get('Dropout')}\n",
    "\n",
    "dense_2 : {best_hps.get('dense_2')}\n",
    "alpha : {best_hps.get('alpha')}\n",
    "\"\"\")\n",
    "# pool_size : {best_hps.get('pool_size')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c44791a-70c3-413f-b850-03edeef2f04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"\"\"\\nconv_2_filter : {best_hps.get(\\'conv_2_filter\\')}\\nconv_2_kernel : {best_hps.get(\\'conv_2_kernel\\')}\\nconv_3_filter : {best_hps.get(\\'conv_3_filter\\')}\\nconv_3_kernel : {best_hps.get(\\'conv_3_kernel\\')}\\n\"\"\")\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(f\"\"\"\n",
    "conv_2_filter : {best_hps.get('conv_2_filter')}\n",
    "conv_2_kernel : {best_hps.get('conv_2_kernel')}\n",
    "conv_3_filter : {best_hps.get('conv_3_filter')}\n",
    "conv_3_kernel : {best_hps.get('conv_3_kernel')}\n",
    "\"\"\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d34068-f036-4a12-ba8e-b22f5a55f77d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f72efa0-c899-4732-8d7c-261252869481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train의 shape:  (246, 5, 8) , X_test의 shape: (62, 5, 8) , y_train의 shape:  (246, 36) , y_test의 shape: (62, 36)\n",
      "X_train의 shape:  (196, 5, 8) , X_test의 shape: (50, 5, 8) , y_train의 shape:  (196, 36) , y_test의 shape: (50, 36)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2D, Y, test_size=0.2, random_state=42)\n",
    "print('X_train의 shape: ', X_train.shape ,', X_test의 shape:',X_test.shape,\n",
    "     ', y_train의 shape: ', y_train.shape ,', y_test의 shape:',y_test.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val  = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print('X_train의 shape: ', X_train.shape ,', X_test의 shape:',X_val.shape,\n",
    "     ', y_train의 shape: ', y_train.shape ,', y_test의 shape:',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa4f8227-f759-495e-8c80-685632714f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dropout, Flatten, Dense, Conv2D, MaxPooling2D, BatchNormalization, Activation\n",
    "\n",
    "def model_2D_CNN(hp):\n",
    "    # create model object\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    input_shape = (5,8,1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [2,3,4]),\n",
    "        # activation='relu',\n",
    "        input_shape = input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('conv_2_filter', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_2_kernel', values = [2,3,4]),\n",
    "        # activation='relu',\n",
    "        padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=hp.Int('conv_3_filter', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_3_kernel', values = [2,3,4]),\n",
    "        # activation='relu',\n",
    "        padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    '''\n",
    "    model.add(MaxPooling2D(pool_size=hp.Int('MaxPooling2D', min_value=1, max_value=5, step=1)\n",
    "                          ))\n",
    "    '''\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_1', min_value=32, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('Dropout', min_value=0.00, max_value=0.5, step=0.05)))    \n",
    "        \n",
    "\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_2', min_value=32, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dense(36,\n",
    "                     activation=keras.layers.LeakyReLU(alpha=hp.Float('alpha', min_value=0.00, max_value=5, step=0.05))\n",
    "                    ))\n",
    "    \n",
    "    #compilation of modelv\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b31485f-e674-4684-8cfc-14fcaef238f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_2D_CNN,\n",
    "                     objective = 'val_loss', \n",
    "                     max_epochs = 10,\n",
    "                     overwrite=True,\n",
    "                     factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43a5e441-5860-46fc-942e-bd4e7214ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 Complete [00h 00m 02s]\n",
      "val_loss: 1.3647934198379517\n",
      "\n",
      "Best val_loss So Far: 1.3436399698257446\n",
      "Total elapsed time: 00h 00m 22s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs = 100, batch_size=24,  validation_data = (X_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=20)])\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e51438de-6f5d-487d-984a-8a0be7d74e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv_1_filter : 32\n",
      "conv_1_kernel : 2\n",
      "conv_2_filter : 96\n",
      "conv_2_kernel : 3\n",
      "conv_3_filter : 48\n",
      "conv_3_kernel : 3\n",
      "\n",
      "dense_1 : 112\n",
      "Dropout : 0.0\n",
      "\n",
      "dense_2 : 48\n",
      "alpha : 1.1500000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "conv_1_filter : {best_hps.get('conv_1_filter')}\n",
    "conv_1_kernel : {best_hps.get('conv_1_kernel')}\n",
    "conv_2_filter : {best_hps.get('conv_2_filter')}\n",
    "conv_2_kernel : {best_hps.get('conv_2_kernel')}\n",
    "conv_3_filter : {best_hps.get('conv_3_filter')}\n",
    "conv_3_kernel : {best_hps.get('conv_3_kernel')}\n",
    "\n",
    "dense_1 : {best_hps.get('dense_1')}\n",
    "Dropout : {best_hps.get('Dropout')}\n",
    "\n",
    "dense_2 : {best_hps.get('dense_2')}\n",
    "alpha : {best_hps.get('alpha')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36a657d2-9272-44bb-9ce5-9bbd69a8d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2D_CNN(hp):\n",
    "    # create model object\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # adding first convolutional layer \n",
    "    model.add(keras.layers.Conv2D(\n",
    "        filters= hp.Int('conv_1', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n",
    "        activation='relu',\n",
    "        input_shape=(5,8,1)\n",
    "    ))\n",
    "\n",
    "    # adding flatten layer    \n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_1', min_value=32, max_value=128, step=16),\n",
    "        activation= 'relu'\n",
    "    ))\n",
    "    # adding dropout layer \n",
    "    model.add(Dropout(hp.Float('Dropout', min_value=0.00, max_value=0.5, step=0.05)))\n",
    "    \n",
    "    # adding dense layer    \n",
    "    model.add(keras.layers.Dense(\n",
    "        units=hp.Int('dense_2', min_value=32, max_value=128, step=16),\n",
    "        activation= 'relu'\n",
    "    ))\n",
    "    # output layer    \n",
    "    model.add(Dense(36, keras.layers.LeakyReLU(alpha=hp.Float('alpha', min_value=0.00, max_value=0.5, step=0.05))))\n",
    "    \n",
    "    #compilation of model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdae7f6c-2417-4ef0-8ed3-53a34b9f2f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_2D_CNN,\n",
    "                     objective = 'val_loss', \n",
    "                     max_epochs = 10,\n",
    "                     overwrite=True,\n",
    "                     factor = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4746753-4e6d-4e9c-b646-8fd85c5067fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 Complete [00h 00m 02s]\n",
      "val_loss: 0.996882975101471\n",
      "\n",
      "Best val_loss So Far: 0.9418453574180603\n",
      "Total elapsed time: 00h 00m 16s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs = 100, batch_size=24,  validation_data = (X_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping('val_loss', patience=20)])\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8985862-c58a-47fc-9f82-da6dafdeb069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv_1_filter : 48\n",
      "conv_1_kernel : 3\n",
      "\n",
      "dense_1 : 48\n",
      "Dropout : 0.05\n",
      "dense_2 : 96\n",
      "alpha : 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "conv_1_filter : {best_hps.get('conv_1')}\n",
    "conv_1_kernel : {best_hps.get('conv_1_kernel')}\n",
    "\n",
    "dense_1 : {best_hps.get('dense_1')}\n",
    "Dropout : {best_hps.get('Dropout')}\n",
    "dense_2 : {best_hps.get('dense_2')}\n",
    "alpha : {best_hps.get('alpha')}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
